{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-26T06:01:27.983386Z","iopub.execute_input":"2024-06-26T06:01:27.983764Z","iopub.status.idle":"2024-06-26T06:01:29.649855Z","shell.execute_reply.started":"2024-06-26T06:01:27.983731Z","shell.execute_reply":"2024-06-26T06:01:29.648862Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')\n#train_data.shape\n#train_data.describe()\n#train_data.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:01:29.651524Z","iopub.execute_input":"2024-06-26T06:01:29.651930Z","iopub.status.idle":"2024-06-26T06:01:29.682774Z","shell.execute_reply.started":"2024-06-26T06:01:29.651902Z","shell.execute_reply":"2024-06-26T06:01:29.681803Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(train_data.columns)\nprint(train_data.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.stripplot(data = train_data, x = 'Survived', y = 'Age', hue = 'Sex', dodge = True, jitter = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(data = train_data, x = 'Survived', y = 'Age', hue = 'Pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data = train_data, x = 'Survived', hue = 'Sex')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_columns = [x for x in train_data.columns if train_data[x].dtype == 'object']\nprint(cat_columns)\nprint(train_data.info())\ntemp_data = pd.get_dummies(train_data)\nnumerical_data = train_data.drop(cat_columns, axis = 1, inplace = False)\nsns.heatmap(temp_data.corr(), annot = True, cmap = 'coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_columns = [x for x in train_data.columns if train_data[x].dtype == 'object']\ncat_frame = train_data[cat_columns]\nprint(cat_frame.info())\ndumb_frame = pd.get_dummies(cat_frame)\nnumerical_data = train_data.drop(cat_columns, axis = 1, inplace = False)\ntemp_frame = pd.concat([dumb_frame, numerical_data])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:01:37.337675Z","iopub.execute_input":"2024-06-26T06:01:37.338501Z","iopub.status.idle":"2024-06-26T06:01:37.485312Z","shell.execute_reply.started":"2024-06-26T06:01:37.338453Z","shell.execute_reply":"2024-06-26T06:01:37.484206Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   Name      891 non-null    object\n 1   Sex       891 non-null    object\n 2   Ticket    891 non-null    object\n 3   Cabin     204 non-null    object\n 4   Embarked  889 non-null    object\ndtypes: object(5)\nmemory usage: 34.9+ KB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"sns.heatmap(temp_frame.corr(), annot = True, cmap = 'coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T06:01:42.397551Z","iopub.execute_input":"2024-06-26T06:01:42.397978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:58:20.803213Z","iopub.execute_input":"2024-06-26T05:58:20.803643Z","iopub.status.idle":"2024-06-26T05:58:20.898197Z","shell.execute_reply.started":"2024-06-26T05:58:20.803605Z","shell.execute_reply":"2024-06-26T05:58:20.896155Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nColumns: 1724 entries, Name_Abbing, Mr. Anthony to Embarked_S\ndtypes: bool(1724)\nmemory usage: 1.5 MB\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"'''#returns a dataframe with only entries that have a null values in a particular field\ndef null_entries(field):\n    null_values = train_data[[field]].isnull()\n    null_values = null_values[null_values[field] == True]\n    return null_values\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x= train_data['Age'], y = train_data['Fare'], hue = train_data['Survived'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x = 'Age', y = 'Fare', hue = 'Survived', data = train_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.barplot(x=train_data['Survived'],y = train_data['Age'],hue = train_data['Pclass'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.barplot(x = \"Sex\", y = \"Survived\", data = train_data, errorbar=('ci', 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model imports\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_score, validation_curve, learning_curve\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_curve(model, x,y, folds,pname, plist):\n    train_score, valid_score = validation_curve(model, x, y, cv = folds, scoring = 'accuracy',param_name=pname, param_range=plist)\n    train_means = np.mean(train_score, axis=1)\n    valid_means=np.mean(valid_score,axis=1)\n    \n    plt.plot(plist, train_means, label = \"Training Accuracy\", color = 'b')\n    plt.plot(plist, valid_means, label = \"Validation Accuracy\", color = 'g')\n    plt.legend(loc = 'best')\n    plt.title('Validation Curve')\n    plt.xlabel(pname)\n    plt.ylabel('Accuracy')\n    plt.tight_layout()\n    plt.show()\n    \ndef learn_curve(model, x, y, folds=5, size_list=[100,200,300,400,500,600]):\n    tsizes,train_score, valid_score = learning_curve(model, x, y,train_sizes = size_list)\n    train_means = np.mean(train_score, axis = 1)\n    valid_means = np.mean(valid_score, axis = 1)\n\n    plt.plot(tsizes, train_means, label = \"Training Accuracy\", color = 'b')\n    plt.plot(tsizes, valid_means, label = \"Validation Accuracy\", color = 'g')\n    plt.legend(loc = 'best')\n    plt.title('Learning Curve as accuracy over training sample size')\n    plt.xlabel('Training samples')\n    plt.ylabel('Accuracy')\n    plt.tight_layout()\n    plt.show()    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data splitting and processing\ny = train_data['Survived']\nx = train_data.copy()\nx = x.drop(['Survived'], axis = 1)\n#print(x.dtypes)\n#print(y.dtypes)\nx_train, x_valid, y_train, y_valid = train_test_split(x,y,test_size = 0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_without = x.drop(['Name','PassengerId'], axis = 1)\n#cat_columns = [c for c in x.columns if x.dtypes[c] == object]\n#print(cat_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining pipelines\ncat_encoding = OneHotEncoder(handle_unknown = 'ignore')\nage_imputer = SimpleImputer(strategy = 'mean')\ncabin_imputer = SimpleImputer(strategy = 'constant', fill_value = 'NaN')\nembarked_imputer = SimpleImputer(strategy = 'constant', fill_value = 'None')\n\ncabin_pipeline = Pipeline(steps=[('cabimp', cabin_imputer),('cabhot', cat_encoding)])\nembarked_pipeline = Pipeline(steps=[('embimp', embarked_imputer),('embhot', cat_encoding)])\n\ncat_columns = ['Name','Sex','Ticket']\ncat_columns2 = ['Sex','Ticket']\ntrans = ColumnTransformer(transformers = [('age', age_imputer, ['Age']),('cabin', cabin_pipeline, ['Cabin']), ('embarked', embarked_pipeline, ['Embarked']), ('onehot', cat_encoding, cat_columns)])\n#cat_columns = [c for c in x.columns if x.dtypes[c] == object]\ntrans_without = ColumnTransformer(transformers = [('age', age_imputer, ['Age']),('cabin', cabin_pipeline, ['Cabin']), ('embarked', embarked_pipeline, ['Embarked']), ('onehot', cat_encoding, cat_columns2)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import sklearn\nprint(sklearn.metrics.get_scorer_names())\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model = RandomForestClassifier(n_estimators = 100)\npipe = Pipeline(steps = [('pre', trans_without), ('model', model)])\n\n\nscore = cross_val_score(pipe, X = x_without, y = y, cv = 5, scoring= 'accuracy')\nprint(score.mean())'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nmodel = RandomForestClassifier(n_estimators = 100)\npipe = Pipeline(steps = [('pre', trans_without), ('model', model)])\npipe.fit(x_without,y)\npassengers = test_data['PassengerId']\ntest_data = test_data.drop(['PassengerId','Name'],axis=1)\npredictions = pipe.predict(test_data)\noutput = pd.DataFrame({'PassengerId':passengers, 'Survived':predictions})\noutput.to_csv('submission.csv',index = False)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nmodel = RandomForestClassifier(n_estimators = 100)\npipe = Pipeline(steps = [('pre', trans), ('model', model)])\nlearn_curve(pipe, x, y, folds = 5,size_list = [100,200,300,400,500,600])\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''mod = RandomForestClassifier()\nencoded_x = trans.fit_transform(x)\n#pipe = Pipeline(steps = [('pre', trans), ('model', mod)])\nvalid_curve(mod, encoded_x, y, folds=5, pname='n_estimators',plist=[5,10,20,50,100,200])'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''model = svm.SVC()\nencoded_x = trans.fit_transform(x)\nvalid_curve(model, encoded_x, y, folds=5,pname = 'kernel', plist=['linear','sigmoid','rbf','polynomial'])\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nmodel = svm.SVC(kernel='linear')\npipe = Pipeline(steps = [('pre', trans), ('model', model)])\n#learn_curve(pipe, x, y)\n#score = cross_val_score(pipe, X = x, y = y, cv = 5, scoring= 'accuracy')\npipe2 = Pipeline(steps = [('pre', trans_without), ('model', model)])\n#scores2 = cross_val_score(pipe2, X = x_without, y = y, cv = 5, scoring= 'accuracy')\n#print(score.mean())\n#print(scores2.mean())\n\npipe2.fit(x_without,y)\npassengers = test_data['PassengerId']\ntest_without = test_data.drop(['PassengerId', 'Name'],axis=1)\npredictions = pipe2.predict(test_without)\noutput = pd.DataFrame({'PassengerId': passengers, 'Survived': predictions})\noutput.to_csv('submission.csv', index = False)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n#mutual info\nfrom sklearn.feature_selection import mutual_info_classif\n\ndef plot_mi(scores):\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \n    plt.figure(dpi=100, figsize = (8,5))\n    plt.show()\n\nx2 = x.copy()\n\nage_imputer = SimpleImputer(strategy = 'mean')\nx2['Age'] = age_imputer.fit_transform(pd.DataFrame(x2['Age']))\n\nfor c in x2.select_dtypes(\"object\"):\n    x2[c], _ = x2[c].factorize()\n    \ndisc_feats = x2.dtypes == int\n\nscores = mutual_info_classif(x2,y, discrete_features=disc_feats)\nscores = pd.Series(scores,name = \"MI scores\", index = x2.columns)\nscores = scores.sort_values(ascending=False)\n\nprint(scores)\nplot_mi(scores)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''from xgboost import XGBClassifier\n\n#encoded_x = trans.fit_transform(x)\n#encoded_xwithout = trans_without.fit_transform(x_without)\n#model = XGBClassifier()\n#valid_curve(model, encoded_xwithout, y, folds=5, pname='n_estimators',plist=[10, 20, 50, 75, 100,200])\nmodel2 = XGBClassifier(n_estimators = 50)\n#score = cross_val_score(model2, X = encoded_x, y = y, cv = 5, scoring= 'accuracy')\n#score_without = score = cross_val_score(model2, X = encoded_xwithout, y = y, cv = 5, scoring= 'accuracy')\n#print(score.mean())\n#print(score_without.mean())\npassengers = test_data['PassengerId']\npipe = Pipeline(steps = [('encode', trans),('model', model2)])\npipe.fit(x,y)\n\npredictions = pipe.predict(test_data)\n\noutput = pd.DataFrame({'PassengerId': passengers, 'Survived': predictions})\noutput.to_csv('submission.csv',index = False)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef flow_model(prehead, inputs):\n    body = keras.Sequential([\n    layers.Dense(units=32, activation  = 'relu', input_shape = [processed_x.shape[1]]),\n    layers.Dense(units = 32, activation = 'relu'),\n    layers.Dense(units = 32, activation = 'relu'),\n    layers.Dense(units = 1)\n    ])\n    preinputs = prehead(inputs)\n    result = body(preinputs)\n    model = tf.keras.Model(inputs, result)\n    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])\n#try processed x for now, think about validation after initial run\n\n#lookie = layers.StringLookup(output_mode = 'one_hot')\n#lookie.adapt(x)\n\n#changing the datatypes to tensorflow equivalents\ninputs = {}\nfeatures = x_without.copy()\nfor name, column in features.items():\n    d = column.dtype\n    #string/object dtypes converted to string\n    if d == object:\n        d = tf.string\n    #numeric columns converted to float32\n    else:\n        d = tf.float32\n    inputs[name] = tf.keras.Input(shape=(1,), name = name, dtype=d)\n    \n    \nnumerics = {name:inp for name,inp in inputs.items() if inp.dtype==tf.float32} #contains all the data for numeric columns in a dictionary\ntempx = layers.Concatenate()(list(numerics.values()))\nnormie = layers.Normalization()\nnormie.adapt(np.array(feature[numerics.keys()]))\nnumeric_inputs = normie(tempx)\npreprocessed_inputs = [numeric_inputs] # this is where we will store our preprocessed data, since numerics columns are done they are placed here first\n\nfor name, col in features.items():\n    if col.dtype == tf.float32:\n        continue\n    \n    lookie = layers.StringLookup(vocabulary = np.unique())\n    one_hot = layers.CategoryEncoding(num_tokens = lookie.vocabulary_size)\n    temps = lookie(col)\n    temps = one_hot(temps)\n    preprocessed_inputs.append(temps)\n\n#at this point preprocessed inputs should be complete\n\n\n\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])\n\nhistory = model.fit(\n    x = processed_x,\n    y = y,\n   # validation_split = 0.2,\n    epochs = 20,\n    verbose = 1,\n)\n\nhistory_frame = pd.DataFrame(history)\nhistory_frame.loc[:,['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}